#############
###SPARK 2###
#############

#Examples from Definitive guide

val flightData = spark.read.option("inferSchema","true").option("header","true").csv("/user/chaitanyapolipalli/flight-data/csv/2010-summary.csv")

#sort

val flightDataSort = flightData.sort("count").show //ascending order
val flightDataSortDesc = flightData.sort($"count".desc).show //descending order

spark.conf.set("spark.sql.shuffle.partitions","5")

#Registering data frame as table or view
flightData.createOrReplaceTempView("flight_data") 
val sqlWay = spark.sql("select DEST_COUNTRY_NAME, count(1) from flight_data group by DEST_COUNTRY_NAME")

val dfWay = flightData.groupBy("DEST_COUNTRY_NAME").count()

#Using max function

spark.sql("select max(count) from flight_data").take(1)

flightData.select(max("count")).take(1)

#Find the top five destination countries in the data

spark.sql("select DEST_COUNTRY_NAME, sum(count) as destination_total from flight_data group by DEST_COUNTRY_NAME order by destination_total desc limit 5").show

flightData.groupBy("DEST_COUNTRY_NAME").sum("count").withColumnRenamed("sum(count)","destination_total").sort($"destination_total".desc).limit(5).show

or 

flightData.groupBy("DEST_COUNTRY_NAME").sum("count").withColumnRenamed("sum(count)","destination_total").sort(desc("destination_total")).limit(5).show

#Case Class Example

case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String,count: BigInt)

val flightsDF = spark.read.parquet("/user/chaitanyapolipalli/flight-data/parquet/2010-summary.parquet/")

val flights = flightsDF.as[Flight]

#How to create dataframes on the fly
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}

val myManualSchema = new StructType(Array(new StructField("some", StringType, true),new StructField("col", StringType, true),new StructField("names", LongType, false)))

val myRows = Seq(Row("Hello", null, 1L))
val myRDD = spark.sparkContext.parallelize(myRows)
val myDf = spark.createDataFrame(myRDD, myManualSchema)

myDf.show()

#Examples of 'select' and 'selectExpr' methods

df.select("DEST_COUNTRY_NAME").show(2)

You can select multiple columns by using the same style of query, just add more column name strings to your select method call:
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)

#Multiple ways of printing column

import org.apache.spark.sql.functions.{expr, col, column}

df.select(
	df.col("DEST_COUNTRY_NAME"),
	col("DEST_COUNTRY_NAME"),
	column("DEST_COUNTRY_NAME"),
	'DEST_COUNTRY_NAME,
	$"DEST_COUNTRY_NAME",
	expr("DEST_COUNTRY_NAME"))
.show(2)

#One common error is attempting to mix Column objects and strings. For example, the following code will result in a compiler error:

df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")

#As we’ve seen thus far, expr is the most flexible reference that we can use. It can refer to a plain column or a string manipulation of a column. To illustrate, let’s change the column name, and then change it back by using the AS keyword and then the alias method on the column:

df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)

#This changes the column name to “destination.” You can further manipulate the result of your expression as another expression:

df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME")).show(2)

#The preceding operation changes the column name back to its original name.Because select followed by a series of expr is such a common pattern, Spark has a shorthand for doing this efficiently: selectExpr. This is probably the most convenient interface for everyday use:

df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)

#This opens up the true power of Spark. We can treat selectExpr as a simple way to build up complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating SQL statement, and as long as the columns resolve, it will be valid! Here’s a simple example that adds a new column withinCountry to our DataFrame that specifies whether the destination and origin are the same:

df.selectExpr("*","(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(2)

#With select expression, we can also specify aggregations over the entire DataFrame by taking advantage of the functions that we have. These look just like what we have been showing so far:

df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show

#Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new column). This might be a constant value or something we’ll need to compare to later on.The way we do this is through literals. This is basically a translation from a given programming language’s literal value to one that Spark understands. Literals are expressions and you can use them in the same way: 

import org.apache.spark.sql.functions.lit
df.select(expr("*"), lit(1).as("One")).show(2)

#There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the withColumn method on our DataFrame. For example, let’s add a column that just adds the number one as a column:

df.withColumn("numberOne", lit(1)).show(2)
df.withColumn("withInCountry",expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show

#Renaming columns
df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").show

#Escaping column names appropriately. In Spark, we do this by using backtick(`) characters.In this example, however, we need to use backticks because we’re referencing a column in an expression:
val dfWithLongColName = df.withColumn("This Long Column-Name",expr("ORIGIN_COUNTRY_NAME"))
dfWithLongColName.selectExpr("`This Long Column-Name`","`This Long Column-Name` as `new col`").show(2)

#By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:

set spark.sql.caseSensitive true

#Removing columns
df.drop("ORIGIN_COUNTRY_NAME")
dfWithLongColName.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")

#Changing column type 
df.withColumn("count2", col("count").cast("long"))

#Filtering rows - below commands will yield same results
df.filter(col("count") < 2).show(2)
df.where("count < 2").show(2)

#Instinctually, you might want to put multiple filters into the same expression. Although this is possible,it is not always useful, because Spark automatically performs all filtering operations at the same time regardless of the filter ordering. This means that if you want to specify multiple AND filters, just chain them sequentially and let Spark handle the rest:
#In Scala, you must use the =!= operator so that you don’t just compare the unevaluated column expression to a string but instead to the evaluated one:
df.where(col("count") < 2).where(col("ORIGIN_COUNTRY_NAME") =!= "Croatia").show(2)


#Getting distinct/unique rows
df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count()
df.select("ORIGIN_COUNTRY_NAME").distinct().count()

#When we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order:

df.sort("count").show(5)
df.orderBy("count", "DEST_COUNTRY_NAME").show(5)
df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5)

#To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a column. These allow you to specify the order in which a given column should be sorted:

df.orderBy(expr("count desc")).show(2)
df.orderBy(desc("count"), asc("DEST_COUNTRY_NAME")).show(2)

#For optimization purposes, it’s sometimes advisable to sort within each partition before another set of transformations. You can use the sortWithinPartitions method to do this:

spark.read.json("/data/flight-data/json/*-summary.json").sortWithinPartitions("count")

#Get number of Repartitions:
df.rdd.getNumPartitions

#Repartition on specific column
df.repartition(col("DEST_COUNTRY_NAME"))

#Repartition with specific number on a specific column
df.repartition(5, col("DEST_COUNTRY_NAME"))

#Coalesce
df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)

/*Functions on Booleans*/
#Filter condition example
val df = spark.read.format("csv").option("inferSchema","true").option("header","true").load("/user/chaitanyapolipalli/retail-data/by-day/2010-12-01.csv")

df.where(col("InvoiceNo").equalTo(536365)).select("InvoiceNo","Description").show(false)
or 
df.where(col("InvoiceNo") === 536365).select("InvoiceNo", "Description").show(false)
or
df.where("InvoiceNo = 536365").show(false)

#Complex filter example
val priceFilter = col("UnitPrice") > 600
val descripFilter = col("Description").contains("POSTAGE")
df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter)).show()

-- in SQL
SELECT * FROM dfTable WHERE StockCode in ("DOT") AND(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)

or

val DOTCodeFilter = col("StockCode") === "DOT"
val priceFilter = col("UnitPrice") > 600
val descripFilter = col("Description").contains("POSTAGE")
df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter))).where("isExpensive").select("unitPrice", "isExpensive").show(false)

-- in SQL
SELECT UnitPrice, (StockCode = 'DOT' AND
(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)) as isExpensive
FROM dfTable
WHERE (StockCode = 'DOT' AND
(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1))


#One “gotcha” that can come up is if you’re working with null data when creating Boolean expressions. If there is a null in your data, you’ll need to treat things a bit differently. Here’s how you can ensure that you perform a null-safe equivalence test:

df.where(col("Description").eqNullSafe("hello")).show()

/*Functions on Numbers*/
#numerical function 'pow' function example

val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"),2) + 5
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)

#The round function rounds up if you’re exactly in between two numbers. You can round down by using the bround:
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)

#Another common task is to compute summary statistics for a column or set of columns. We can use the describe method to achieve exactly this. This will take all numeric columns and calculate the count, mean, standard deviation, min, and max.

df.describe().show()

#As a last note, we can also add a unique ID to each row by using the function monotonically_increasing_id. This function generates a unique value for each row, starting with 0:
df.select(monotonically_increasing_id()).show(2)

/*Functions on Strings*/
#The initcap function will capitalize every word in a given string when that word is separated from another by a space
df.select(initcap(col("Description"))).show(2, false)

df.select(col("Description"),lower(col("Description")),upper(lower(col("Description")))).show(2)

#Another trivial task is adding or removing spaces around a string. You can do this by using lpad, ltrim, rpad and rtrim, trim:
#Note that if lpad or rpad takes a number less than the length of the string, it will always remove values from the right side of the string.
df.select(
ltrim(lit(" HELLO ")).as("ltrim"),
rtrim(lit(" HELLO ")).as("rtrim"),
trim(lit(" HELLO ")).as("trim"),
lpad(lit("HELLO"), 3, " ").as("lp"),
rpad(lit("HELLO"), 10, " ").as("rp")).show(2)

/*Regular Expressions*/

val simpleColors = Seq("black", "white", "red", "green", "blue")
val regexString = simpleColors.map(_.toUpperCase).mkString("|")
// the | signifies `OR` in regular expression syntax
df.select(regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),col("Description")).show(false)

#Another task might be to replace given characters with other characters. Building this as a regular expression could be tedious, so Spark also provides the translate function to replace these values. This is done at the character level and will replace all instances of a character with the indexed character in the replacement string:

df.select(translate(col("Description"), "LEET", "1337"), col("Description")).show(2)

#regexp_extract example
val regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")
// the | signifies OR in regular expression syntax
df.select(regexp_extract(col("Description"), regexString, 1).alias("color_clean"),col("Description")).show(2)

#contains example
val containsBlack = col("Description").contains("BLACK")
val containsWhite = col("DESCRIPTION").contains("WHITE")
df.withColumn("hasSimpleColor", containsBlack.or(containsWhite)).where("hasSimpleColor").select("Description").show(3, false)

/*Dates Examples*/
val dateDF = spark.range(10).withColumn("today", current_date()).withColumn("now", current_timestamp())dateDF.createOrReplaceTempView("dateTable")

#add 5 days and subtract five days
dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show()

#difference between months and days
dateDF.withColumn("week_ago", date_sub(col("today"), 7)).select(datediff(col("today"), col("week_ago"))).show()
dateDF.select(to_date(lit("2016-01-01")).alias("start"),to_date(lit("2017-05-22")).alias("end")).select(months_between(col("end"), col("start"))).show()

#The to_date function allows you to convert a string to a date, optionally with a specified format. to_date (optionally requires a format) and to_timestamp (always requires a format) examples

#to_date example
val dateFormat = "yyyy-dd-MM"
val cleanDateDF = spark.range(1).select(to_date(lit("2017-12-11"), dateFormat).alias("date"),to_date(lit("2017-20-12"), dateFormat).alias("date2"))

cleanDateDF.createOrReplaceTempView("dateTable2")

#to_timestamp example
cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()

# "drop" function. The simplest function is drop, which removes rows that contain nulls. The default is to drop any row in which any value is null:
#Specifying "any" as an argument drops a row if any of the values are null. Using “all” drops the row only if all values are null or NaN for that row:

df.na.drop("any") 
df.na.drop("all") 

#We can also apply this to certain sets of columns by passing in an array of columns:

df.na.drop("all", Seq("StockCode", "InvoiceNo"))

# "fill" example. Using the fill function, you can fill one or more columns with a set of values. This can be done by specifying a map—that is a particular value and a set of columns. For example, to fill all null values in columns of type String, you might specify the following: 
df.na.fill("All Null values become this string")
#We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for Doubles
df.na.fill(5:Double). 

#To specify columns, we just pass in an array of column names like we did in the previous example:

df.na.fill(5, Seq("StockCode", "InvoiceNo"))

#We can also do this with with a Scala Map, where the key is the column name and the value is the value we would like to use to fill null values:

val fillColValues = Map("StockCode" -> 5, "Description" -> "No Value")
df.na.fill(fillColValues)

# "replace" example. In addition to replacing null values like we did with drop and fill, there are more flexible options that you can use with more than just null values. Probably the most common use case is to replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value:

df.na.replace("Description", Map("" -> "UNKNOWN"))


/*Working with Complex Types*/

# "Structs" Example

#You can think of structs as DataFrames within DataFrames. A worked example will illustrate this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query:
df.selectExpr("(Description, InvoiceNo) as complex", "*")
df.selectExpr("struct(Description, InvoiceNo) as complex", "*")

val complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))
complexDF.createOrReplaceTempView("complexDF")

# We now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method getField:
complexDF.select("complex.Description")
complexDF.select(col("complex").getField("Description"))

#We can also query all values in the struct by using *. This brings up all the columns to the top-level DataFrame:
complexDF.select("complex.*")

# "array" example. To define arrays, let’s work through a use case. With our current data, our objective is to take every single word in our Description column and convert that into a row in our DataFrame. The first task is to turn our Description column into a complex type, an array.

#We do this by using the split function and specify the delimiter:

df.select(split(col("Description"), " ")).show(2)

# "explode" example. The explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array.

df.withColumn("splitted", split(col("Description"), " ")).withColumn("exploded", explode(col("splitted"))).select("Description", "InvoiceNo", "exploded").show(2)

# "Map" example

df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(false)

#You can also explode map types, which will turn them into columns: 
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("explode(complex_map)").show(false)

/*Working with JSON types*/

#You can use the get_json_object to inline query a JSON object, be it a dictionary or array. You can use json_tuple if this object has only one level of nesting:
val jsonDF = spark.range(1).selectExpr("""'{"myJSONKey" : {"myJSONValue" : [1, 2, 3]}}' as jsonString""")
jsonDF.select(get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]") as "column",json_tuple(col("jsonString"), "myJSONKey")).show(2)

#You can also turn a StructType into a JSON string by using the to_json function:
df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct")))

#This function also accepts a dictionary (map) of parameters that are the same as the JSON data source. You can use the from_json function to parse this (or other JSON data) back in. This naturally requires you to specify a schema, and optionally you can specify a map of options, as well:
val parseSchema = new StructType(Array(
new StructField("InvoiceNo",StringType,true),
new StructField("Description",StringType,true)))

df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct")).alias("newJSON")).select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2)

/*User Defined Functions*/
#UDFs can take and return one or more columns as input. Spark UDFs are incredibly powerful because you can write them in several different programming languages; you do not need to create them in an esoteric format or domain-specific language. They’re just functions that operate on the data, record by record. By default, these functions are registered as temporary functions to be used in that specific SparkSession or Context. Although you can write UDFs in Scala, Python, or Java, there are performance considerations that you should be aware of. To illustrate this, we’re going to walk through exactly what happens when you create UDF, pass that into Spark, and then execute code using that UDF.

#When you use the function, there are essentially two different things that occur. If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects; we cover that in the section on optimization in Chapter 19. If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark. 

#Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark cannot manage the memory of the worker. This means that you could potentially cause a worker to fail if it becomes resource constrained (because both the JVM and Python are competing for memory on the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of time it should take you to write the function in Scala will always yield significant speed ups, and on top of that, you can still use the function from Python!

val udfExampleDF = spark.range(5).toDF("num")

def power3(number:Double):Double = number * number * number

val power3udf = udf(power3(_:Double):Double)

udfExampleDF.select(power3udf(col("num"))).show()

#At this juncture, we can use this only as a DataFrame function. That is to say, we can’t use it within a string expression, only on an expression. However, we can also register this UDF as a Spark SQL function. This is valuable because it makes it simple to use this function within SQL as well as across languages. Let’s register the function in Scala:

spark.udf.register("power3", power3(_:Double):Double)
udfExampleDF.selectExpr("power3(num)").show(2)

#Because this function is registered with Spark SQL—and we’ve learned that any Spark SQL function or expression is valid to use as an expression when working with DataFrames—we can turn around and use the UDF that we wrote in Scala, in Python. However, rather than using it as a DataFrame function, we use it as a SQL expression

#As a last note, you can also use UDF/UDAF creation via a Hive syntax. To allow for this, first you must enable Hive support when they create their SparkSession (via SparkSession.builder().enableHiveSupport()). Then you can register UDFs in SQL. This is only supported with precompiled Scala and Java packages, so you’ll need to specify them as a dependency:
-- in SQL

CREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName'

#Additionally, you can register this as a permanent function in the Hive Metastore by removing TEMPORARY.

/*Aggregations*/

# "count" example
#There are a number of gotchas when it comes to null values and counting. For instance, when performing a count(*), Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values.
val df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/data/retail-data/all/*.csv").coalesce(5)
df.cache()
df.createOrReplaceTempView("dfTable")

df.select(count("StockCode")).show()

# "countDistinct" example
#Sometimes, the total number is not relevant; rather, it’s the number of unique groups that you want. To get this number, you can use the countDistinct function. This is a bit more relevant for individual columns:
df.select(countDistinct("StockCode")).show()

# "approx_count_distinct" example
#Often, we find ourselves working with large datasets and the exact distinct count is irrelevant. There are times when an approximation to a certain degree of accuracy will work just fine, and for that, you can use the approx_count_distinct function:

#You will notice that approx_count_distinct took another parameter with which you can specify the maximum estimation error allowed. In this case, we specified a rather large error and thus receive an answer that is quite far off but does complete more quickly than countDistinct. You will see much greater performance gains with larger datasets. 
df.select(approx_count_distinct("StockCode", 0.1)).show()

# "first" and "last" example
df.select(first("StockCode"), last("StockCode")).show()

# "min" and "max" example
df.select(min("Quantity"), max("Quantity")).show()

# "sum" example
df.select(sum("Quantity")).show()

# "sumDistinct" example. In addition to summing a total, you also can sum a distinct set of values by using the sumDistinct function:
df.select(sumDistinct("Quantity")).show()

/*Aggregating to Complex Types*/

#In Spark, you can perform aggregations not just of numerical values using formulas, you can also perform them on complex types. For example, we can collect a list of values present in a given column or only the unique values by collecting to a set. You can use this to carry out some more programmatic access later on in the pipeline or pass the entire collection in a user-defined function (UDF):

df.agg(collect_set("Country"), collect_list("Country")).show()

# "groupBy" example
df.groupBy("InvoiceNo", "CustomerId").count().show()

# Grouping with Expressions. As we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the count function. Rather than passing that function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like alias a column after transforming it for later use in your data flow:
df.groupBy("InvoiceNo").agg(count("Quantity").alias("quan"),expr("count(Quantity)")).show()

# "cube", "rollup" and "groupBy" examples
https://stackoverflow.com/questions/37975227/what-is-the-difference-between-cube-rollup-and-groupby-operators

/* Types of Joins*/
#Inner joins (keep rows with keys that exist in the left and right datasets)
#Outer joins (keep rows with keys in either the left or right datasets)
#Left outer joins (keep rows with keys in the left dataset)
#Right outer joins (keep rows with keys in the right dataset)
#Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)
#Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)
#Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)
#Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)


#Examples:
val person = Seq((0, "Bill Chambers", 0, Seq(100)),(1, "Matei Zaharia", 1, Seq(500, 250, 100)),(2, "Michael Armbrust", 1, Seq(250, 100))).toDF("id", "name", "graduate_program","spark_status")
val graduateProgram = Seq((0, "Masters", "School of Information", "UC Berkeley"),(2, "Masters", "EECS", "UC Berkeley"),(1, "Ph.D.", "EECS", "UC Berkeley")).toDF("id", "degree","department", "school")
val sparkStatus = Seq((500, "Vice President"),(250, "PMC Member"),(100, "Contributor")).toDF("id", "status")

person.createOrReplaceTempView("person")
graduateProgram.createOrReplaceTempView("graduateProgram")
sparkStatus.createOrReplaceTempView("sparkStatus")

/*Inner Join*/
val joinExpression = person.col("graduate_program") === graduateProgram.col("id")
person.join(graduateProgram, joinExpression).show()

#We can also specify this explicitly by passing in a third parameter, the joinType:
var joinType = "inner"
person.join(graduateProgram, joinExpression, joinType).show()

/*Outer Join*/
var joinType = "outer"
person.join(graduateProgram,joinExpression,joinType).show(false)

/*Left Outer Joins*/
var joinType = "left_outer"
graduateProgram.join(person,joinExpression,joinType).show(false)

/*Right Outer Joins*/
var joinType = "right_outer"
person.join(graduateProgram,joinExpression,joinType).show(false)

/*Left Semi Joins*/
#Semi joins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join:

joinType = "left_semi"
person.join(graduateProgram,joinExpression,joinType).show(false)

#To prove it returns duplicates as well
val gradProgram2 = graduateProgram.union(Seq((0, "Masters", "Duplicated Row", "Duplicated School")).toDF())
gradProgram2.createOrReplaceTempView("gradProgram2")
gradProgram2.join(person, joinExpression, joinType).show()

/*Left Anti Joins*/
#Left anti joins are the opposite of left semi joins. Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a corresponding key in the second DataFrame. Think of anti joins as a NOT IN SQL-style filter:
joinType = "left_anti"
graduateProgram.join(person, joinExpression, joinType).show()

/*Cross Joins*/
joinType = "cross"

graduateProgram.join(person, joinExpression, joinType).show()
or
person.crossJoin(graduateProgram).show()

/*Joins on Complex Types*/
person.withColumnRenamed("id", "personId").join(sparkStatus, expr("array_contains(spark_status, id)")).show()

/*Handling Duplicate Column Names*/
#One of the tricky things that come up in joins is dealing with duplicate column names in your results DataFrame. In a DataFrame, each column has a unique ID within Spark’s SQL Engine, Catalyst. This unique ID is purely internal and not something that you can directly reference. This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names. This can occur in two distinct situations: 

#The join expression that you specify does not remove one key from one of the input DataFrames and the keys have the same column name
#Two columns on which you are not performing the join have the same name

#Let’s create a problem dataset that we can use to illustrate these problems:
val gradProgramDupe = graduateProgram.withColumnRenamed("id", "graduate_program")
val joinExpr = gradProgramDupe.col("graduate_program") === person.col("graduate_program")

#Note that there are now two graduate_program columns, even though we joined on that key:
person.join(gradProgramDupe, joinExpr).show()

#The challenge arises when we refer to one of these columns:
person.join(gradProgramDupe, joinExpr).select("graduate_program").show()

#Given the previous code snippet, we will receive an error. In this particular example, Spark generates this message:
org.apache.spark.sql.AnalysisException: Reference 'graduate_program' is ambiguous, could be: graduate_program#40, graduate_program#1079.;

#Approach 1: Different join expression
#When you have two keys that have the same name, probably the easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of the columns for you during the join:
person.join(gradProgramDupe,"graduate_program").select("graduate_program").show()

#Approach 2: Dropping the column after the join
#Another approach is to drop the offending column after the join. When doing this, we need to refer to the column via the original source DataFrame. We can do this if the join uses the same key names or if the source DataFrames have columns that simply have the same name:
person.join(gradProgramDupe, joinExpr).drop(person.col("graduate_program")).select("graduate_program").show()
val joinExpr = person.col("graduate_program") === graduateProgram.col("id")
person.join(graduateProgram, joinExpr).drop(graduateProgram.col("id")).show()

#This is an artifact of Spark’s SQL analysis process in which an explicitly referenced column will pass analysis because Spark has no need to resolve the column. Notice how the column uses the .col method instead of a column function. That allows us to implicitly specify that column by its specific ID.
#Approach 3: Renaming a column before the join We can avoid this issue altogether if we rename one of our columns before the join:
val gradProgram3 = graduateProgram.withColumnRenamed("id", "grad_id")
val joinExpr = person.col("graduate_program") === gradProgram3.col("grad_id")
person.join(gradProgram3, joinExpr).show()

/*Spark Read*/

#The core structure for reading data is as follows:
DataFrameReader.format(...).option("key", "value").schema(...).load()

#Eg:
spark.read.format("csv")
.option("mode", "FAILFAST")
.option("inferSchema", "true")
.option("path", "path/to/file(s)")
.schema(someSchema)
.load()

import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
val myManualSchema = new StructType(Array(new StructField("DEST_COUNTRY_NAME", StringType, true),new StructField("ORIGIN_COUNTRY_NAME", StringType, true),new StructField("count", LongType, false)))

val csvFile = spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/user/chaitanyapolipalli/flight-data/csv/2010-summary.csv")

/*Read Modes*/
permissive(default) -- 	Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record
dropMalformed 		--	Drops the row that contains malformed records
failFast 			--	Fails immediately upon encountering malformed records

/*Spark Write*/
#core structure for writing data is as follows:
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()

/*Save Modes*/
append 			-- 	Appends the output files to the list of files that already exist at that location
overwrite 		-- 	Will completely overwrite any data that already exists there
errorIfExists(default) 	--	Throws an error and fails the write if data or files already exist at the specified location
ignore 			-- 	If data or files exist at the location, do nothing with the current DataFrame

#Eg:
dataframe.write.format("csv")
.option("mode", "OVERWRITE")
.option("dateFormat", "yyyy-MM-dd")
.option("path", "path/to/file(s)")
.save()

#saving as tab seperated file (.tsv)
csvFile.write.format("csv").mode("overwrite").option("sep", "\t").save("/user/chaitanyapolipalli/definitive_guide_examples/tmp/my-tsv-file.tsv")

#CSV Read Example
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
val myManualSchema = new StructType(Array(new StructField("DEST_COUNTRY_NAME", StringType, true),new StructField("ORIGIN_COUNTRY_NAME", StringType, true),new StructField("count", LongType, false)))
spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/csv/2010-summary.csv").show(5)

#CSV Write Example
csvFile.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/my-tsv-file.tsv")

#JSON Read Example
spark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema).load("/data/flight-data/json/2010-summary.json").show(5)

#JSON Write Example
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")

#Parquet Read Example
spark.read.format("parquet").load("/data/flight-data/parquet/2010-summary.parquet").show(5)

#Parquet Write Example
csvFile.write.format("parquet").mode("overwrite").save("/tmp/my-parquet-file.parquet")

#ORC Read Example
spark.read.format("orc").load("/data/flight-data/orc/2010-summary.orc").show(5)

#ORC Write Example
csvFile.write.format("orc").mode("overwrite").save("/tmp/my-json-file.orc")

/*Reading from SQL Databases*/
#Connecting to MySql Database in itversity

spark-shell --jars /usr/share/java/mysql-connector-java.jar 
val dbDataFrame = spark.read.format("jdbc").option("driver", "com.mysql.jdbc.Driver").option("url", "jdbc:mysql://ms.itversity.com:3306/retail_export").option("dbtable", "chaitanyapolipalli_flight_data").option("user","retail_user").option("password","itversity").load()

val driver = "com.mysql.jdbc.Driver"
val url = "jdbc:mysql://ms.itversity.com:3306/retail_export"
val tablename = "chaitanyapolipalli_flight_data"
val user = "retail_user"
val password = "itversity"


dbDataFrame.select("DEST_COUNTRY_NAME").distinct.show

#Spark makes a best-effort attempt to filter data in the database itself before creating the DataFrame. For example, in the previous sample query, we can see from the query plan that it selects only the relevant column name from the table:
dbDataFrame.select("DEST_COUNTRY_NAME").distinct.explain
== Physical Plan ==
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#46, 200)
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[])
      +- *(1) Scan JDBCRelation(chaitanyapolipalli_flight_data) [numPartitions=1] [DEST_COUNTRY_NAME#46] PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
	  
#Spark can actually do better than this on certain queries. For example, if we specify a filter on our DataFrame, Spark will push that filter down into the database.
dbDataFrame.filter("DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')").explain

val pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM chaitanyapolipalli_flight_data)AS flight_info"""
val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", pushdownQuery).option("driver", driver).option("user",user).option("password",password).load()

#Partitioning a file while reading from DB
val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", tablename).option("driver", driver).option("numPartitions", 10).option("user",user).option("password",password).load()

#Specifying predicates while reading from DB
val props = new java.util.Properties
props.setProperty("driver", driver)
props.setProperty("user", user)
props.setProperty("password", password)
val predicates = Array("DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'","DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'")
spark.read.jdbc(url, tablename, predicates, props).show()
spark.read.jdbc(url, tablename, predicates, props).rdd.getNumPartitions

/*Writing to SQL Databases*/
flightData.write.format("jdbc").option("url","jdbc:mysql://ms.itversity.com:3306/retail_export").
option("driver","com.mysql.jdbc.Driver").option("dbtable","chaitanyapolipalli_flight_data").option("user","retail_user").option("password","itversity").mode("append").save()

/*Text Files*/

#Reading
spark.read.textFile("/user/chaitanyapolipalli/flight-data/csv/2010-summary.csv").selectExpr("split(value, ',') as rows").show(false)

#Writing
val csvFile = spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/user/chaitanyapolipalli/flight-data/csv/2010-summary.csv")
csvFile.select("DEST_COUNTRY_NAME").write.text("/user/chaitanyapolipalli/tmp/simple-text-file.txt")

#If you perform some partitioning when performing your write (we’ll discuss partitioning in the next couple of pages), you can write more columns. However, those columns will manifest as directories in the folder to which you’re writing out to, instead of columns on every single file:
csvFile.limit(10).select("DEST_COUNTRY_NAME", "count").write.partitionBy("count").text("/user/chaitanyapolipalli/tmp/five-csv-files2.csv")

/*Advanced I/O Concepts*/

#Partitioning
#Partitioning is a tool that allows you to control what data is stored (and where) as you write it. When you write a file to a partitioned directory (or table), you basically encode a column as a folder. What this allows you to do is skip lots of data when you go to read it in later, allowing you to read in only the data relevant to your problem instead of having to scan the complete dataset. These are supported for all file-based data sources:

csvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME").save("/tmp/partitioned-files.parquet")

#Bucketing
#Bucketing is another file organization approach with which you can control the data that is specifically written to each file. This can help avoid shuffles later when you go to read the data because data with the same bucket ID will all be grouped together into one physical partition. This means that the data is prepartitioned according to how you expect to use that data later on, meaning you can avoid expensive shuffles when joining or aggregating. Rather than partitioning on a specific column (which might write out a ton of directories), it’s probably worthwhile to explore bucketing the data instead. This will create a certain number of files and organize our data into those “buckets”:
val numberBuckets = 10
val columnToBucketBy = "count"
csvFile.write.format("parquet").mode("overwrite").bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles")

#Spark 2.2 introduced a new method for controlling file sizes in a more automatic way. We saw previously that the number of output files is a derivative of the number of partitions we had at write time (and the partitioning columns we selected). Now, you can take advantage of another tool in order to limit output file sizes so that you can target an optimum file size. You can use the maxRecordsPerFile option and specify a number of your choosing. This allows you to better control file sizes by controlling the number of records that are written to each file. For example, if you set an option for a writer as df.write.option("maxRecordsPerFile", 5000), Spark will ensure that files will contain at most 5,000 records.

/*Spark SQL*/

#Spark SQL is intended to operate as an online analytic processing (OLAP) database, not an online transaction processing (OLTP) database. This means that it is not intended to perform extremely low-latency queries. Even though support for inplace modifications is sure to be something that comes up in the future, it’s not something that is currently available.

/*Creating Tables*/
CREATE TABLE flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG) USING JSON OPTIONS (path '/data/flight-data/json/2015-summary.json')

#You can also add comments to certain columns in a table, which can help other developers understand  the data in the tables:
CREATE TABLE IF NOT EXISTS flights_csv (DEST_COUNTRY_NAME STRING,ORIGIN_COUNTRY_NAME STRING COMMENT "remember, the US will be most prevalent",count LONG) USING csv OPTIONS (header true, path'/data/flight-data/csv/2015-summary.csv')

#You can control the layout of the data by writing out a partitioned dataset
CREATE TABLE partitioned_flights USING parquet PARTITIONED BY (DEST_COUNTRY_NAME) AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5

/*Describing Table Metadata*/
DESCRIBE TABLE flights_csv
SHOW PARTITIONS partitioned_flights

/*Refreshing Table Metadata*/
REFRESH table partitioned_flights
MSCK REPAIR TABLE partitioned_flights

/*Dropping Tables*/
DROP TABLE IF EXISTS flights_csv;

/*Caching Tables*/
#Caching
CACHE TABLE flights

#Uncaching
UNCACHE TABLE FLIGHTS

/*Creating Views*/
CREATE VIEW just_usa_view AS SELECT * FROM flights WHERE dest_country_name = 'United States'

#Creating temporary view
CREATE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'

#Creating global temp view
CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'

#Creating or replacing a view
CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'

#Dropping views
DROP VIEW IF EXISTS just_usa_view;

/*Databases*/
#Examples
SHOW DATABASES
CREATE DATABASE some_db
USE some_db
SHOW tables
SELECT * FROM default.flights
SELECT current_database() #You can see what database you’re currently using
DROP DATABASE IF EXISTS some_db;

/*case…when…then Statements*/
SELECT
	CASE 
		 WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1
		 WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0
		 ELSE -1 
	END
FROM partitioned_flights

/*Complex Types*/
#Structs

CREATE VIEW IF NOT EXISTS nested_data AS SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights

#You can even query individual columns within a struct—all you need to do is use dot syntax:
SELECT country.DEST_COUNTRY_NAME, count FROM nested_data

/*Functions*/
SHOW FUNCTIONS
SHOW SYSTEM FUNCTIONS
SHOW USER FUNCTIONS
SHOW FUNCTIONS "s*";
SHOW FUNCTIONS LIKE "collect*";

/*Subqueries*/

SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5

SELECT * FROM flights WHERE origin_country_name IN (SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)

SELECT * FROM flights f1 WHERE EXISTS (SELECT 1 FROM flights f2 WHERE f1.dest_country_name = f2.origin_country_name) AND EXISTS (SELECT 1 FROM flights f2 WHERE f2.dest_country_name = f1.origin_country_name) 

SELECT *, (SELECT max(count) FROM flights) AS maximum FROM flights

/*DataSets*/

#To begin creating a Dataset, let’s define a case class for one of our datasets:
case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)
val flightsDF = spark.read.parquet("/user/chaitanyapolipalli/flight-data/parquet/2010-summary.parquet/")
val flights = flightsDF.as[Flight]
flights.show(2)

#You’ll also notice that when we actually go to access one of the case classes, we don’t need to do any type coercion, we simply specify the named attribute of the case class and get back, not just the expected value but the expected type, as well:
flights.first.DEST_COUNTRY_NAME

/*Transformations*/
def originIsDestination(flight_row: Flight): Boolean = {
return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME
}

flights.filter(flight_row => originIsDestination(flight_row)).first()

#The result is:
Flight = Flight(United States,United States,348113)

#As we saw earlier, this function does not need to execute in Spark code at all. Similar to our UDFs, we can use it and test it on data on our local machines before using it within Spark. For example, this dataset is small enough for us to collect to the driver (as an Array of Flights) on which we can operate and perform the exact same filtering operation:
flights.collect().filter(flight_row => originIsDestination(flight_row))

#The result is:
Array[Flight] = Array(Flight(United States,United States,348113))

#We can see that we get the exact same answer as before.

/*Mapping*/
val destinations = flights.map(f => f.DEST_COUNTRY_NAME)
val localDestinations = destinations.take(5)

/*Joins*/
case class FlightMetadata(count: BigInt, randomData: BigInt)
val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong)).withColumnRenamed("_1", "count").withColumnRenamed("_2", "randomData").as[FlightMetadata]
val flights2 = flights.joinWith(flightsMeta, flights.col("count") === flightsMeta.col("count"))

#Notice that we end up with a Dataset of a sort of key-value pair, in which each row represents a Flight and the Flight Metadata. We can, of course, query these as a Dataset or a DataFrame with complex types:
flights2.selectExpr("_1.DEST_COUNTRY_NAME")
#We can collect them just as we did before:
flights2.take(2)

/*Grouping and Aggregations*/
#Grouping and aggregations follow the same fundamental standards that we saw in the previous aggregation chapter, so groupBy rollup and cube still apply, but these return DataFrames instead of Datasets (you lose type information):

flights.groupBy("DEST_COUNTRY_NAME").count()
#This often is not too big of a deal, but if you want to keep type information around there are other groupings and aggregations that you can perform. An excellent example is the groupByKey method. This allows you to group by a specific key in the Dataset and get a typed Dataset in return. This function, however, doesn’t accept a specific column name but rather a function. This makes it possible for you to specify more sophisticated grouping functions that are much more akin to something like this:
flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()

#It should be straightfoward enough to understand that this is a more expensive process than aggregating immediately after scanning, especially because it ends up in the same end result:
#This should motivate using Datasets only with user-defined encoding surgically and only where it makes sense. This might be at the beginning of a big data pipeline or at the end of one.

/*Resilient Distributed Datasets(RDDs)*/

val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")
val words = spark.sparkContext.parallelize(myCollection, 2)
#Set name for RDD
words.setName("myWords")

#Get naem of RDD
words.name

#Reading data from data sources
spark.sparkContext.textFile("/some/path/withTextFiles")
spark.sparkContext.wholeTextFiles("/some/path/withTextFiles")

#Checkpointing
spark.sparkContext.setCheckpointDir("/some/path/for/checkpointing")
words.checkpoint()

#When we are in the key–value pair format, we can also extract the specific keys or values by using the following methods:
keyword.keys.collect()
keyword.values.collect()

#'lookup' method example
keyword.lookup("s")

#coalesce
Coalesce effectively collapses partitions on the same worker in order to avoid a shuffle of the data when repartitioning. For instance, our words RDD is currently two partitions, we can collapse that to one partition by using coalesce without bringing about a shuffle of the data:
words.coalesce(1).getNumPartitions

#repartition
The repartition operation allows you to repartition your data up or down but performs a shuffle across nodes in the process. Increasing the number of partitions can increase the level of parallelism when operating in map- and filter-type operations:
words.repartition(10)

/*Distributed Shared Variables*/

#Broadcast Variables

val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")
val words = spark.sparkContext.parallelize(myCollection, 2)
val supplementalData = Map("Spark" -> 1000, "Definitive" -> 200,"Big" -> -300, "Simple" -> 100)

val suppBroadcast = spark.sparkContext.broadcast(supplementalData)
suppBroadcast.value

words.map(word => (word, suppBroadcast.value.getOrElse(word, 0))).sortBy(wordPair => wordPair._2).collect()

#Accumulators
case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String,count: BigInt)

val flightsDF = spark.read.parquet("/user/chaitanyapolipalli/flight-data/parquet/2010-summary.parquet/")

val flights = flightsDF.as[Flight]

#Unnamed Accumulator
import org.apache.spark.util.LongAccumulator
val accUnnamed = new LongAccumulator
val acc = spark.sparkContext.register(accUnnamed)

#Named Accumulator
val accChina = new LongAccumulator
val accChina2 = spark.sparkContext.longAccumulator("China")
spark.sparkContext.register(accChina, "China")

flights.foreach(flight_row => accChinaFunc(flight_row))
accChina.value

#Custom Accumulators
Although Spark does provide some default accumulator types, sometimes you might want to build your own custom accumulator. In order to do this you need to subclass the AccumulatorV2 class. There are several abstract methods that you need to implement, as you can see in the example that follows.
import scala.collection.mutable.ArrayBuffer
import org.apache.spark.util.AccumulatorV2
val arr = ArrayBuffer[BigInt]()
class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {
private var num:BigInt = 0
def reset(): Unit = {
this.num = 0
}
def add(intValue: BigInt): Unit = {
if (intValue % 2 == 0) {
this.num += intValue
}
}
def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {
this.num += other.value
}
def value():BigInt = {
this.num
}
def copy(): AccumulatorV2[BigInt,BigInt] = {
new EvenAccumulator
}
def isZero():Boolean = {
this.num == 0
}
}
val acc = new EvenAccumulator
val newAcc = sc.register(acc, "evenAcc")
// in Scala
acc.value // 0
flights.foreach(flight_row => acc.add(flight_row.count))
acc.value // 31390






















